# api_server.py
# æ–°ä¸€ä»£ LMArena Bridge åç«¯æœåŠ¡

import asyncio
import json
import logging
import os
import sys
import subprocess
import time
import uuid
import re
import threading
import random
import mimetypes
from datetime import datetime
from contextlib import asynccontextmanager

import uvicorn
import requests
from packaging.version import parse as parse_version
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse, Response

# --- å†…éƒ¨æ¨¡å—å¯¼å…¥ ---
from modules.file_uploader import upload_to_file_bed


# --- åŸºç¡€é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- å…¨å±€çŠ¶æ€ä¸é…ç½® ---
CONFIG = {} # å­˜å‚¨ä» config.jsonc åŠ è½½çš„é…ç½®
# browser_ws ç”¨äºå­˜å‚¨ä¸å•ä¸ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚
# æ³¨æ„ï¼šæ­¤æ¶æ„å‡å®šåªæœ‰ä¸€ä¸ªæµè§ˆå™¨æ ‡ç­¾é¡µåœ¨å·¥ä½œã€‚
# å¦‚æœéœ€è¦æ”¯æŒå¤šä¸ªå¹¶å‘æ ‡ç­¾é¡µï¼Œéœ€è¦å°†æ­¤æ‰©å±•ä¸ºå­—å…¸ç®¡ç†å¤šä¸ªè¿æ¥ã€‚
browser_ws: WebSocket | None = None
# response_channels ç”¨äºå­˜å‚¨æ¯ä¸ª API è¯·æ±‚çš„å“åº”é˜Ÿåˆ—ã€‚
# é”®æ˜¯ request_idï¼Œå€¼æ˜¯ asyncio.Queueã€‚
response_channels: dict[str, asyncio.Queue] = {}
last_activity_time = None # è®°å½•æœ€åä¸€æ¬¡æ´»åŠ¨çš„æ—¶é—´
idle_monitor_thread = None # ç©ºé—²ç›‘æ§çº¿ç¨‹
main_event_loop = None # ä¸»äº‹ä»¶å¾ªç¯
# æ–°å¢ï¼šç”¨äºè·Ÿè¸ªæ˜¯å¦å› äººæœºéªŒè¯è€Œåˆ·æ–°
IS_REFRESHING_FOR_VERIFICATION = False


# --- æ¨¡å‹æ˜ å°„ ---
# MODEL_NAME_TO_ID_MAP ç°åœ¨å°†å­˜å‚¨æ›´ä¸°å¯Œçš„å¯¹è±¡ï¼š { "model_name": {"id": "...", "type": "..."} }
MODEL_NAME_TO_ID_MAP = {}
MODEL_ENDPOINT_MAP = {} # æ–°å¢ï¼šç”¨äºå­˜å‚¨æ¨¡å‹åˆ° session/message ID çš„æ˜ å°„
DEFAULT_MODEL_ID = None # é»˜è®¤æ¨¡å‹id: None

def load_model_endpoint_map():
    """ä» model_endpoint_map.json åŠ è½½æ¨¡å‹åˆ°ç«¯ç‚¹çš„æ˜ å°„ã€‚"""
    global MODEL_ENDPOINT_MAP
    try:
        with open('model_endpoint_map.json', 'r', encoding='utf-8') as f:
            content = f.read()
            # å…è®¸ç©ºæ–‡ä»¶
            if not content.strip():
                MODEL_ENDPOINT_MAP = {}
            else:
                MODEL_ENDPOINT_MAP = json.loads(content)
        logger.info(f"æˆåŠŸä» 'model_endpoint_map.json' åŠ è½½äº† {len(MODEL_ENDPOINT_MAP)} ä¸ªæ¨¡å‹ç«¯ç‚¹æ˜ å°„ã€‚")
    except FileNotFoundError:
        logger.warning("'model_endpoint_map.json' æ–‡ä»¶æœªæ‰¾åˆ°ã€‚å°†ä½¿ç”¨ç©ºæ˜ å°„ã€‚")
        MODEL_ENDPOINT_MAP = {}
    except json.JSONDecodeError as e:
        logger.error(f"åŠ è½½æˆ–è§£æ 'model_endpoint_map.json' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ç©ºæ˜ å°„ã€‚")
        MODEL_ENDPOINT_MAP = {}

def _parse_jsonc(jsonc_string: str) -> dict:
    """
    ç¨³å¥åœ°è§£æ JSONC å­—ç¬¦ä¸²ï¼Œç§»é™¤æ³¨é‡Šã€‚
    """
    lines = jsonc_string.splitlines()
    no_comments_lines = []
    in_block_comment = False
    for line in lines:
        stripped_line = line.strip()
        if in_block_comment:
            if '*/' in stripped_line:
                in_block_comment = False
                line = stripped_line.split('*/', 1)[1]
            else:
                continue
        
        if '/*' in line and not in_block_comment:
            before_comment, _, after_comment = line.partition('/*')
            if '*/' in after_comment:
                _, _, after_block = after_comment.partition('*/')
                line = before_comment + after_block
            else:
                line = before_comment
                in_block_comment = True

        if line.strip().startswith('//'):
            continue
        
        no_comments_lines.append(line)

    return json.loads("\n".join(no_comments_lines))

def load_config():
    """ä» config.jsonc åŠ è½½é…ç½®ï¼Œå¹¶å¤„ç† JSONC æ³¨é‡Šã€‚"""
    global CONFIG
    try:
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            content = f.read()
        CONFIG = _parse_jsonc(content)
        logger.info("æˆåŠŸä» 'config.jsonc' åŠ è½½é…ç½®ã€‚")
        # æ‰“å°å…³é”®é…ç½®çŠ¶æ€
        logger.info(f"  - é…’é¦†æ¨¡å¼ (Tavern Mode): {'âœ… å¯ç”¨' if CONFIG.get('tavern_mode_enabled') else 'âŒ ç¦ç”¨'}")
        logger.info(f"  - ç»•è¿‡æ¨¡å¼ (Bypass Mode): {'âœ… å¯ç”¨' if CONFIG.get('bypass_enabled') else 'âŒ ç¦ç”¨'}")
    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½æˆ–è§£æ 'config.jsonc' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨é»˜è®¤é…ç½®ã€‚")
        CONFIG = {}

def load_model_map():
    """ä» models.json åŠ è½½æ¨¡å‹æ˜ å°„ï¼Œæ”¯æŒ 'id:type' æ ¼å¼ã€‚"""
    global MODEL_NAME_TO_ID_MAP
    try:
        with open('models.json', 'r', encoding='utf-8') as f:
            raw_map = json.load(f)
            
        processed_map = {}
        for name, value in raw_map.items():
            if isinstance(value, str) and ':' in value:
                parts = value.split(':', 1)
                model_id = parts[0] if parts[0].lower() != 'null' else None
                model_type = parts[1]
                processed_map[name] = {"id": model_id, "type": model_type}
            else:
                # é»˜è®¤æˆ–æ—§æ ¼å¼å¤„ç†
                processed_map[name] = {"id": value, "type": "text"}

        MODEL_NAME_TO_ID_MAP = processed_map
        logger.info(f"æˆåŠŸä» 'models.json' åŠ è½½å¹¶è§£æäº† {len(MODEL_NAME_TO_ID_MAP)} ä¸ªæ¨¡å‹ã€‚")

    except (FileNotFoundError, json.JSONDecodeError) as e:
        logger.error(f"åŠ è½½ 'models.json' å¤±è´¥: {e}ã€‚å°†ä½¿ç”¨ç©ºæ¨¡å‹åˆ—è¡¨ã€‚")
        MODEL_NAME_TO_ID_MAP = {}

# --- å…¬å‘Šå¤„ç† ---
def check_and_display_announcement():
    """æ£€æŸ¥å¹¶æ˜¾ç¤ºä¸€æ¬¡æ€§å…¬å‘Šã€‚"""
    announcement_file = "announcement-lmarena.json"
    if os.path.exists(announcement_file):
        try:
            logger.info("="*60)
            logger.info("ğŸ“¢ æ£€æµ‹åˆ°æ›´æ–°å…¬å‘Šï¼Œå†…å®¹å¦‚ä¸‹:")
            with open(announcement_file, 'r', encoding='utf-8') as f:
                announcement = json.load(f)
                title = announcement.get("title", "å…¬å‘Š")
                content = announcement.get("content", [])
                
                logger.info(f"   --- {title} ---")
                for line in content:
                    logger.info(f"   {line}")
                logger.info("="*60)

        except json.JSONDecodeError:
            logger.error(f"æ— æ³•è§£æå…¬å‘Šæ–‡ä»¶ '{announcement_file}'ã€‚æ–‡ä»¶å†…å®¹å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„JSONã€‚")
        except Exception as e:
            logger.error(f"è¯»å–å…¬å‘Šæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            try:
                os.remove(announcement_file)
                logger.info(f"å…¬å‘Šæ–‡ä»¶ '{announcement_file}' å·²è¢«ç§»é™¤ã€‚")
            except OSError as e:
                logger.error(f"åˆ é™¤å…¬å‘Šæ–‡ä»¶ '{announcement_file}' å¤±è´¥: {e}")

# --- æ›´æ–°æ£€æŸ¥ ---
GITHUB_REPO = "Lianues/LMArenaBridge"

def download_and_extract_update(version):
    """ä¸‹è½½å¹¶è§£å‹æœ€æ–°ç‰ˆæœ¬åˆ°ä¸´æ—¶æ–‡ä»¶å¤¹ã€‚"""
    update_dir = "update_temp"
    if not os.path.exists(update_dir):
        os.makedirs(update_dir)

    try:
        zip_url = f"https://github.com/{GITHUB_REPO}/archive/refs/heads/main.zip"
        logger.info(f"æ­£åœ¨ä» {zip_url} ä¸‹è½½æ–°ç‰ˆæœ¬...")
        response = requests.get(zip_url, timeout=60)
        response.raise_for_status()

        # éœ€è¦å¯¼å…¥ zipfile å’Œ io
        import zipfile
        import io
        with zipfile.ZipFile(io.BytesIO(response.content)) as z:
            z.extractall(update_dir)
        
        logger.info(f"æ–°ç‰ˆæœ¬å·²æˆåŠŸä¸‹è½½å¹¶è§£å‹åˆ° '{update_dir}' æ–‡ä»¶å¤¹ã€‚")
        return True
    except requests.RequestException as e:
        logger.error(f"ä¸‹è½½æ›´æ–°å¤±è´¥: {e}")
    except zipfile.BadZipFile:
        logger.error("ä¸‹è½½çš„æ–‡ä»¶ä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„zipå‹ç¼©åŒ…ã€‚")
    except Exception as e:
        logger.error(f"è§£å‹æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    
    return False

def check_for_updates():
    """ä» GitHub æ£€æŸ¥æ–°ç‰ˆæœ¬ã€‚"""
    if not CONFIG.get("enable_auto_update", True):
        logger.info("è‡ªåŠ¨æ›´æ–°å·²ç¦ç”¨ï¼Œè·³è¿‡æ£€æŸ¥ã€‚")
        return

    current_version = CONFIG.get("version", "0.0.0")
    logger.info(f"å½“å‰ç‰ˆæœ¬: {current_version}ã€‚æ­£åœ¨ä» GitHub æ£€æŸ¥æ›´æ–°...")

    try:
        config_url = f"https://raw.githubusercontent.com/{GITHUB_REPO}/main/config.jsonc"
        response = requests.get(config_url, timeout=10)
        response.raise_for_status()

        jsonc_content = response.text
        remote_config = _parse_jsonc(jsonc_content)
        
        remote_version_str = remote_config.get("version")
        if not remote_version_str:
            logger.warning("è¿œç¨‹é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç‰ˆæœ¬å·ï¼Œè·³è¿‡æ›´æ–°æ£€æŸ¥ã€‚")
            return

        if parse_version(remote_version_str) > parse_version(current_version):
            logger.info("="*60)
            logger.info(f"ğŸ‰ å‘ç°æ–°ç‰ˆæœ¬! ğŸ‰")
            logger.info(f"  - å½“å‰ç‰ˆæœ¬: {current_version}")
            logger.info(f"  - æœ€æ–°ç‰ˆæœ¬: {remote_version_str}")
            if download_and_extract_update(remote_version_str):
                logger.info("å‡†å¤‡åº”ç”¨æ›´æ–°ã€‚æœåŠ¡å™¨å°†åœ¨5ç§’åå…³é—­å¹¶å¯åŠ¨æ›´æ–°è„šæœ¬ã€‚")
                time.sleep(5)
                update_script_path = os.path.join("modules", "update_script.py")
                # ä½¿ç”¨ Popen å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹
                subprocess.Popen([sys.executable, update_script_path])
                # ä¼˜é›…åœ°é€€å‡ºå½“å‰æœåŠ¡å™¨è¿›ç¨‹
                os._exit(0)
            else:
                logger.error(f"è‡ªåŠ¨æ›´æ–°å¤±è´¥ã€‚è¯·è®¿é—® https://github.com/{GITHUB_REPO}/releases/latest æ‰‹åŠ¨ä¸‹è½½ã€‚")
            logger.info("="*60)
        else:
            logger.info("æ‚¨çš„ç¨‹åºå·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚")

    except requests.RequestException as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°å¤±è´¥: {e}")
    except json.JSONDecodeError:
        logger.error("è§£æè¿œç¨‹é…ç½®æ–‡ä»¶å¤±è´¥ã€‚")
    except Exception as e:
        logger.error(f"æ£€æŸ¥æ›´æ–°æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# --- æ¨¡å‹æ›´æ–° ---
def extract_models_from_html(html_content):
    """
    ä» HTML å†…å®¹ä¸­æå–å®Œæ•´çš„æ¨¡å‹JSONå¯¹è±¡ï¼Œä½¿ç”¨æ‹¬å·åŒ¹é…ç¡®ä¿å®Œæ•´æ€§ã€‚
    """
    models = []
    model_names = set()
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„æ¨¡å‹JSONå¯¹è±¡çš„èµ·å§‹ä½ç½®
    for start_match in re.finditer(r'\{\\"id\\":\\"[a-f0-9-]+\\"', html_content):
        start_index = start_match.start()
        
        # ä»èµ·å§‹ä½ç½®å¼€å§‹ï¼Œè¿›è¡ŒèŠ±æ‹¬å·åŒ¹é…
        open_braces = 0
        end_index = -1
        
        # ä¼˜åŒ–ï¼šè®¾ç½®ä¸€ä¸ªåˆç†çš„æœç´¢ä¸Šé™ï¼Œé¿å…æ— é™å¾ªç¯
        search_limit = start_index + 10000 # å‡è®¾ä¸€ä¸ªæ¨¡å‹å®šä¹‰ä¸ä¼šè¶…è¿‡10000ä¸ªå­—ç¬¦
        
        for i in range(start_index, min(len(html_content), search_limit)):
            if html_content[i] == '{':
                open_braces += 1
            elif html_content[i] == '}':
                open_braces -= 1
                if open_braces == 0:
                    end_index = i + 1
                    break
        
        if end_index != -1:
            # æå–å®Œæ•´çš„ã€è½¬ä¹‰çš„JSONå­—ç¬¦ä¸²
            json_string_escaped = html_content[start_index:end_index]
            
            # åè½¬ä¹‰
            json_string = json_string_escaped.replace('\\"', '"').replace('\\\\', '\\')
            
            try:
                model_data = json.loads(json_string)
                model_name = model_data.get('publicName')
                
                # ä½¿ç”¨publicNameå»é‡
                if model_name and model_name not in model_names:
                    models.append(model_data)
                    model_names.add(model_name)
            except json.JSONDecodeError as e:
                logger.warning(f"è§£ææå–çš„JSONå¯¹è±¡æ—¶å‡ºé”™: {e} - å†…å®¹: {json_string[:150]}...")
                continue

    if models:
        logger.info(f"æˆåŠŸæå–å¹¶è§£æäº† {len(models)} ä¸ªç‹¬ç«‹æ¨¡å‹ã€‚")
        return models
    else:
        logger.error("é”™è¯¯ï¼šåœ¨HTMLå“åº”ä¸­æ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…çš„å®Œæ•´æ¨¡å‹JSONå¯¹è±¡ã€‚")
        return None

def save_available_models(new_models_list, models_path="available_models.json"):
    """
    å°†æå–åˆ°çš„å®Œæ•´æ¨¡å‹å¯¹è±¡åˆ—è¡¨ä¿å­˜åˆ°æŒ‡å®šçš„JSONæ–‡ä»¶ä¸­ã€‚
    """
    logger.info(f"æ£€æµ‹åˆ° {len(new_models_list)} ä¸ªæ¨¡å‹ï¼Œæ­£åœ¨æ›´æ–° '{models_path}'...")
    
    try:
        with open(models_path, 'w', encoding='utf-8') as f:
            # ç›´æ¥å°†å®Œæ•´çš„æ¨¡å‹å¯¹è±¡åˆ—è¡¨å†™å…¥æ–‡ä»¶
            json.dump(new_models_list, f, indent=4, ensure_ascii=False)
        logger.info(f"âœ… '{models_path}' å·²æˆåŠŸæ›´æ–°ï¼ŒåŒ…å« {len(new_models_list)} ä¸ªæ¨¡å‹ã€‚")
    except IOError as e:
        logger.error(f"âŒ å†™å…¥ '{models_path}' æ–‡ä»¶æ—¶å‡ºé”™: {e}")

# --- è‡ªåŠ¨é‡å¯é€»è¾‘ ---
def restart_server():
    """ä¼˜é›…åœ°é€šçŸ¥å®¢æˆ·ç«¯åˆ·æ–°ï¼Œç„¶åé‡å¯æœåŠ¡å™¨ã€‚"""
    logger.warning("="*60)
    logger.warning("æ£€æµ‹åˆ°æœåŠ¡å™¨ç©ºé—²è¶…æ—¶ï¼Œå‡†å¤‡è‡ªåŠ¨é‡å¯...")
    logger.warning("="*60)
    
    # 1. (å¼‚æ­¥) é€šçŸ¥æµè§ˆå™¨åˆ·æ–°
    async def notify_browser_refresh():
        if browser_ws:
            try:
                # ä¼˜å…ˆå‘é€ 'reconnect' æŒ‡ä»¤ï¼Œè®©å‰ç«¯çŸ¥é“è¿™æ˜¯ä¸€ä¸ªè®¡åˆ’å†…çš„é‡å¯
                await browser_ws.send_text(json.dumps({"command": "reconnect"}, ensure_ascii=False))
                logger.info("å·²å‘æµè§ˆå™¨å‘é€ 'reconnect' æŒ‡ä»¤ã€‚")
            except Exception as e:
                logger.error(f"å‘é€ 'reconnect' æŒ‡ä»¤å¤±è´¥: {e}")
    
    # åœ¨ä¸»äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥é€šçŸ¥å‡½æ•°
    # ä½¿ç”¨`asyncio.run_coroutine_threadsafe`ç¡®ä¿çº¿ç¨‹å®‰å…¨
    if browser_ws and browser_ws.client_state.name == 'CONNECTED' and main_event_loop:
        asyncio.run_coroutine_threadsafe(notify_browser_refresh(), main_event_loop)
    
    # 2. å»¶è¿Ÿå‡ ç§’ä»¥ç¡®ä¿æ¶ˆæ¯å‘é€
    time.sleep(3)
    
    # 3. æ‰§è¡Œé‡å¯
    logger.info("æ­£åœ¨é‡å¯æœåŠ¡å™¨...")
    os.execv(sys.executable, ['python'] + sys.argv)

def idle_monitor():
    """åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œï¼Œç›‘æ§æœåŠ¡å™¨æ˜¯å¦ç©ºé—²ã€‚"""
    global last_activity_time
    
    # ç­‰å¾…ï¼Œç›´åˆ° last_activity_time è¢«é¦–æ¬¡è®¾ç½®
    while last_activity_time is None:
        time.sleep(1)
        
    logger.info("ç©ºé—²ç›‘æ§çº¿ç¨‹å·²å¯åŠ¨ã€‚")
    
    while True:
        if CONFIG.get("enable_idle_restart", False):
            timeout = CONFIG.get("idle_restart_timeout_seconds", 300)
            
            # å¦‚æœè¶…æ—¶è®¾ç½®ä¸º-1ï¼Œåˆ™ç¦ç”¨é‡å¯æ£€æŸ¥
            if timeout == -1:
                time.sleep(10) # ä»ç„¶éœ€è¦ä¼‘çœ ä»¥é¿å…ç¹å¿™å¾ªç¯
                continue

            idle_time = (datetime.now() - last_activity_time).total_seconds()
            
            if idle_time > timeout:
                logger.info(f"æœåŠ¡å™¨ç©ºé—²æ—¶é—´ ({idle_time:.0f}s) å·²è¶…è¿‡é˜ˆå€¼ ({timeout}s)ã€‚")
                restart_server()
                break # é€€å‡ºå¾ªç¯ï¼Œå› ä¸ºè¿›ç¨‹å³å°†è¢«æ›¿æ¢
                
        # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡
        time.sleep(10)

# --- FastAPI ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è¿è¡Œçš„ç”Ÿå‘½å‘¨æœŸå‡½æ•°ã€‚"""
    global idle_monitor_thread, last_activity_time, main_event_loop
    main_event_loop = asyncio.get_running_loop() # è·å–ä¸»äº‹ä»¶å¾ªç¯
    load_config() # é¦–å…ˆåŠ è½½é…ç½®
    
    # --- æ‰“å°å½“å‰çš„æ“ä½œæ¨¡å¼ ---
    mode = CONFIG.get("id_updater_last_mode", "direct_chat")
    target = CONFIG.get("id_updater_battle_target", "A")
    logger.info("="*60)
    logger.info(f"  å½“å‰æ“ä½œæ¨¡å¼: {mode.upper()}")
    if mode == 'battle':
        logger.info(f"  - Battle æ¨¡å¼ç›®æ ‡: Assistant {target}")
    logger.info("  (å¯é€šè¿‡è¿è¡Œ id_updater.py ä¿®æ”¹æ¨¡å¼)")
    logger.info("="*60)

    check_for_updates() # æ£€æŸ¥ç¨‹åºæ›´æ–°
    load_model_map() # é‡æ–°å¯ç”¨æ¨¡å‹åŠ è½½
    load_model_endpoint_map() # åŠ è½½æ¨¡å‹ç«¯ç‚¹æ˜ å°„
    logger.info("æœåŠ¡å™¨å¯åŠ¨å®Œæˆã€‚ç­‰å¾…æ²¹çŒ´è„šæœ¬è¿æ¥...")

    # æ£€æŸ¥å¹¶æ˜¾ç¤ºå…¬å‘Šï¼Œæ”¾åœ¨å¯åŠ¨ä¿¡æ¯çš„æœ€åï¼Œä½¿å…¶æ›´æ˜¾çœ¼
    check_and_display_announcement()

    # åœ¨æ¨¡å‹æ›´æ–°åï¼Œæ ‡è®°æ´»åŠ¨æ—¶é—´çš„èµ·ç‚¹
    last_activity_time = datetime.now()
    
    # å¯åŠ¨ç©ºé—²ç›‘æ§çº¿ç¨‹
    if CONFIG.get("enable_idle_restart", False):
        idle_monitor_thread = threading.Thread(target=idle_monitor, daemon=True)
        idle_monitor_thread.start()
        

    yield
    logger.info("æœåŠ¡å™¨æ­£åœ¨å…³é—­ã€‚")

app = FastAPI(lifespan=lifespan)

# --- CORS ä¸­é—´ä»¶é…ç½® ---
# å…è®¸æ‰€æœ‰æ¥æºã€æ‰€æœ‰æ–¹æ³•ã€æ‰€æœ‰è¯·æ±‚å¤´ï¼Œè¿™å¯¹äºæœ¬åœ°å¼€å‘å·¥å…·æ˜¯å®‰å…¨çš„ã€‚
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- è¾…åŠ©å‡½æ•° ---
def save_config():
    """å°†å½“å‰çš„ CONFIG å¯¹è±¡å†™å› config.jsonc æ–‡ä»¶ï¼Œä¿ç•™æ³¨é‡Šã€‚"""
    try:
        # è¯»å–åŸå§‹æ–‡ä»¶ä»¥ä¿ç•™æ³¨é‡Šç­‰
        with open('config.jsonc', 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å®‰å…¨åœ°æ›¿æ¢å€¼
        def replacer(key, value, content):
            # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼ä¼šæ‰¾åˆ° keyï¼Œç„¶ååŒ¹é…å®ƒçš„ value éƒ¨åˆ†ï¼Œç›´åˆ°é€—å·æˆ–å³èŠ±æ‹¬å·
            pattern = re.compile(rf'("{key}"\s*:\s*").*?("?)(,?\s*)$', re.MULTILINE)
            replacement = rf'\g<1>{value}\g<2>\g<3>'
            if not pattern.search(content): # å¦‚æœ key ä¸å­˜åœ¨ï¼Œå°±æ·»åŠ åˆ°æ–‡ä»¶æœ«å°¾ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                 content = re.sub(r'}\s*$', f'  ,"{key}": "{value}"\n}}', content)
            else:
                 content = pattern.sub(replacement, content)
            return content

        content_str = "".join(lines)
        content_str = replacer("session_id", CONFIG["session_id"], content_str)
        content_str = replacer("message_id", CONFIG["message_id"], content_str)
        
        with open('config.jsonc', 'w', encoding='utf-8') as f:
            f.write(content_str)
        logger.info("âœ… æˆåŠŸå°†ä¼šè¯ä¿¡æ¯æ›´æ–°åˆ° config.jsoncã€‚")
    except Exception as e:
        logger.error(f"âŒ å†™å…¥ config.jsonc æ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)


async def _process_openai_message(message: dict) -> dict:
    """
    å¤„ç†OpenAIæ¶ˆæ¯ï¼Œåˆ†ç¦»æ–‡æœ¬å’Œé™„ä»¶ã€‚
    - å°†å¤šæ¨¡æ€å†…å®¹åˆ—è¡¨åˆ†è§£ä¸ºçº¯æ–‡æœ¬å’Œé™„ä»¶åˆ—è¡¨ã€‚
    - æ–‡ä»¶åºŠé€»è¾‘å·²ç§»è‡³ chat_completions é¢„å¤„ç†ï¼Œæ­¤å¤„ä»…å¤„ç†å¸¸è§„é™„ä»¶æ„å»ºã€‚
    - ç¡®ä¿ user è§’è‰²çš„ç©ºå†…å®¹è¢«æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥é¿å… LMArena å‡ºé”™ã€‚
    """
    content = message.get("content")
    role = message.get("role")
    attachments = []
    text_content = ""

    if isinstance(content, list):
        text_parts = []
        for part in content:
            if part.get("type") == "text":
                text_parts.append(part.get("text", ""))
            elif part.get("type") == "image_url":
                # æ­¤å¤„çš„ URL å¯èƒ½æ˜¯ base64 æˆ– http URL (å·²è¢«é¢„å¤„ç†å™¨æ›¿æ¢)
                image_url_data = part.get("image_url", {})
                url = image_url_data.get("url")
                original_filename = image_url_data.get("detail")

                try:
                    # å¯¹äº base64ï¼Œæˆ‘ä»¬éœ€è¦æå– content_type
                    if url.startswith("data:"):
                        content_type = url.split(';')[0].split(':')[1]
                    else:
                        # å¯¹äº http URLï¼Œæˆ‘ä»¬å°è¯•çŒœæµ‹ content_type
                        content_type = mimetypes.guess_type(url)[0] or 'application/octet-stream'

                    file_name = original_filename or f"image_{uuid.uuid4()}.{mimetypes.guess_extension(content_type).lstrip('.') or 'png'}"
                    
                    attachments.append({
                        "name": file_name,
                        "contentType": content_type,
                        "url": url
                    })

                except (AttributeError, IndexError, ValueError) as e:
                    logger.warning(f"å¤„ç†é™„ä»¶URLæ—¶å‡ºé”™: {url[:100]}... é”™è¯¯: {e}")

        text_content = "\n\n".join(text_parts)
    elif isinstance(content, str):
        text_content = content

    if role == "user" and not text_content.strip():
        text_content = " "

    return {
        "role": role,
        "content": text_content,
        "attachments": attachments
    }

async def convert_openai_to_lmarena_payload(openai_data: dict, session_id: str, message_id: str, mode_override: str = None, battle_target_override: str = None) -> dict:
    """
    å°† OpenAI è¯·æ±‚ä½“è½¬æ¢ä¸ºæ²¹çŒ´è„šæœ¬æ‰€éœ€çš„ç®€åŒ–è½½è·ï¼Œå¹¶åº”ç”¨é…’é¦†æ¨¡å¼ã€ç»•è¿‡æ¨¡å¼ä»¥åŠå¯¹æˆ˜æ¨¡å¼ã€‚
    æ–°å¢äº†æ¨¡å¼è¦†ç›–å‚æ•°ï¼Œä»¥æ”¯æŒæ¨¡å‹ç‰¹å®šçš„ä¼šè¯æ¨¡å¼ã€‚
    """
    # 1. è§„èŒƒåŒ–è§’è‰²å¹¶å¤„ç†æ¶ˆæ¯
    #    - å°†éæ ‡å‡†çš„ 'developer' è§’è‰²è½¬æ¢ä¸º 'system' ä»¥æé«˜å…¼å®¹æ€§ã€‚
    #    - åˆ†ç¦»æ–‡æœ¬å’Œé™„ä»¶ã€‚
    messages = openai_data.get("messages", [])
    for msg in messages:
        if msg.get("role") == "developer":
            msg["role"] = "system"
            logger.info("æ¶ˆæ¯è§’è‰²è§„èŒƒåŒ–ï¼šå°† 'developer' è½¬æ¢ä¸º 'system'ã€‚")
            
    processed_messages = []
    for msg in messages:
        processed_msg = await _process_openai_message(msg.copy())
        processed_messages.append(processed_msg)

    # 2. åº”ç”¨é…’é¦†æ¨¡å¼ (Tavern Mode)
    if CONFIG.get("tavern_mode_enabled"):
        system_prompts = [msg['content'] for msg in processed_messages if msg['role'] == 'system']
        other_messages = [msg for msg in processed_messages if msg['role'] != 'system']
        
        merged_system_prompt = "\n\n".join(system_prompts)
        final_messages = []
        
        if merged_system_prompt:
            # ç³»ç»Ÿæ¶ˆæ¯ä¸åº”æœ‰é™„ä»¶
            final_messages.append({"role": "system", "content": merged_system_prompt, "attachments": []})
        
        final_messages.extend(other_messages)
        processed_messages = final_messages

    # 3. ç¡®å®šç›®æ ‡æ¨¡å‹ ID
    model_name = openai_data.get("model", "claude-3-5-sonnet-20241022")
    model_info = MODEL_NAME_TO_ID_MAP.get(model_name, {}) # å…³é”®ä¿®å¤ï¼šç¡®ä¿ model_info æ€»æ˜¯ä¸€ä¸ªå­—å…¸
    
    target_model_id = None
    if model_info:
        target_model_id = model_info.get("id")
    else:
        logger.warning(f"æ¨¡å‹ '{model_name}' åœ¨ 'models.json' ä¸­æœªæ‰¾åˆ°ã€‚è¯·æ±‚å°†ä¸å¸¦ç‰¹å®šæ¨¡å‹IDå‘é€ã€‚")

    if not target_model_id:
        logger.warning(f"æ¨¡å‹ '{model_name}' åœ¨ 'models.json' ä¸­æœªæ‰¾åˆ°å¯¹åº”çš„IDã€‚è¯·æ±‚å°†ä¸å¸¦ç‰¹å®šæ¨¡å‹IDå‘é€ã€‚")

    # 4. æ„å»ºæ¶ˆæ¯æ¨¡æ¿
    message_templates = []
    for msg in processed_messages:
        message_templates.append({
            "role": msg["role"],
            "content": msg.get("content", ""),
            "attachments": msg.get("attachments", [])
        })
    
    # 4.5. ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœç”¨æˆ·æ¶ˆæ¯ç»“å°¾åŒ…å«--bypassä¸”åŒ…å«å›¾ç‰‡ï¼Œæ„é€ è™šå‡åŠ©æ‰‹æ¶ˆæ¯
    if message_templates and message_templates[-1]["role"] == "user":
        last_msg = message_templates[-1]
        if last_msg["content"].strip().endswith("--bypass") and last_msg.get("attachments"):
            has_images = False
            for attachment in last_msg.get("attachments", []):
                if attachment.get("contentType", "").startswith("image/"):
                    has_images = True
                    break
            
            if has_images:
                logger.info("æ£€æµ‹åˆ°--bypassæ ‡è®°å’Œå›¾ç‰‡é™„ä»¶ï¼Œæ„é€ è™šå‡åŠ©æ‰‹æ¶ˆæ¯")
                
                # ç§»é™¤ç”¨æˆ·æ¶ˆæ¯ä¸­çš„--bypassæ ‡è®°
                last_msg["content"] = last_msg["content"].strip()[:-9].strip()
                
                # æ„é€ ä¸€ä¸ªè™šå‡åŠ©æ‰‹æ¶ˆæ¯ï¼Œä½¿ç”¨ç”¨æˆ·æ¶ˆæ¯ä¸­çš„å›¾ç‰‡é™„ä»¶
                fake_assistant_msg = {
                    "role": "assistant",
                    "content": "",  # ç©ºå†…å®¹
                    "attachments": last_msg.get("attachments", []).copy()  # å¤åˆ¶ç”¨æˆ·çš„å›¾ç‰‡é™„ä»¶
                }
                
                # æ¸…ç©ºåŸç”¨æˆ·æ¶ˆæ¯çš„é™„ä»¶åˆ—è¡¨
                last_msg["attachments"] = []
                
                # å°†è™šå‡åŠ©æ‰‹æ¶ˆæ¯æ’å…¥åˆ°ç”¨æˆ·æ¶ˆæ¯å‰
                message_templates.insert(len(message_templates)-1, fake_assistant_msg)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åœ¨ç¬¬ä¸€ä½æ·»åŠ è™šå‡ç”¨æˆ·æ¶ˆæ¯
                if message_templates[0]["role"] == "assistant":
                    logger.info("æ£€æµ‹åˆ°ç¬¬ä¸€æ¡æ¶ˆæ¯æ˜¯åŠ©æ‰‹æ¶ˆæ¯ï¼Œæ·»åŠ è™šå‡ç”¨æˆ·æ¶ˆæ¯...")
                    fake_user_msg = {
                        "role": "user",
                        "content": "Hi",
                        "attachments": []
                    }
                    message_templates.insert(0, fake_user_msg)

    # 5. åº”ç”¨ç»•è¿‡æ¨¡å¼ (Bypass Mode) - ä»…å¯¹æ–‡æœ¬æ¨¡å‹ç”Ÿæ•ˆ
    model_type = model_info.get("type", "text")
    if CONFIG.get("bypass_enabled") and model_type == "text":
        # ç»•è¿‡æ¨¡å¼æ€»æ˜¯æ·»åŠ ä¸€ä¸ª position 'a' çš„ç”¨æˆ·æ¶ˆæ¯
        logger.info("ç»•è¿‡æ¨¡å¼å·²å¯ç”¨ï¼Œæ­£åœ¨æ³¨å…¥ä¸€ä¸ªç©ºçš„ç”¨æˆ·æ¶ˆæ¯ã€‚")
        message_templates.append({"role": "user", "content": " ", "participantPosition": "a", "attachments": []})

    # 6. åº”ç”¨å‚ä¸è€…ä½ç½® (Participant Position)
    # ä¼˜å…ˆä½¿ç”¨è¦†ç›–çš„æ¨¡å¼ï¼Œå¦åˆ™å›é€€åˆ°å…¨å±€é…ç½®
    mode = mode_override or CONFIG.get("id_updater_last_mode", "direct_chat")
    target_participant = battle_target_override or CONFIG.get("id_updater_battle_target", "A")
    target_participant = target_participant.lower() # ç¡®ä¿æ˜¯å°å†™

    logger.info(f"æ­£åœ¨æ ¹æ®æ¨¡å¼ '{mode}' (ç›®æ ‡: {target_participant if mode == 'battle' else 'N/A'}) è®¾ç½® Participant Positions...")

    for msg in message_templates:
        if msg['role'] == 'system':
            if mode == 'battle':
                # Battle æ¨¡å¼: system ä¸ç”¨æˆ·é€‰æ‹©çš„åŠ©æ‰‹åœ¨åŒä¸€è¾¹ (Aåˆ™a, Båˆ™b)
                msg['participantPosition'] = target_participant
            else:
                # DirectChat æ¨¡å¼: system å›ºå®šä¸º 'b'
                msg['participantPosition'] = 'b'
        elif mode == 'battle':
            # Battle æ¨¡å¼ä¸‹ï¼Œé system æ¶ˆæ¯ä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç›®æ ‡ participant
            msg['participantPosition'] = target_participant
        else: # DirectChat æ¨¡å¼
            # DirectChat æ¨¡å¼ä¸‹ï¼Œé system æ¶ˆæ¯ä½¿ç”¨é»˜è®¤çš„ 'a'
            msg['participantPosition'] = 'a'

    return {
        "message_templates": message_templates,
        "target_model_id": target_model_id,
        "session_id": session_id,
        "message_id": message_id
    }

# --- OpenAI æ ¼å¼åŒ–è¾…åŠ©å‡½æ•° (ç¡®ä¿JSONåºåˆ—åŒ–ç¨³å¥) ---
def format_openai_chunk(content: str, model: str, request_id: str) -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI æµå¼å—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"

def format_openai_finish_chunk(model: str, request_id: str, reason: str = 'stop') -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI ç»“æŸå—ã€‚"""
    chunk = {
        "id": request_id, "object": "chat.completion.chunk",
        "created": int(time.time()), "model": model,
        "choices": [{"index": 0, "delta": {}, "finish_reason": reason}]
    }
    return f"data: {json.dumps(chunk, ensure_ascii=False)}\n\ndata: [DONE]\n\n"

def format_openai_error_chunk(error_message: str, model: str, request_id: str) -> str:
    """æ ¼å¼åŒ–ä¸º OpenAI é”™è¯¯å—ã€‚"""
    content = f"\n\n[LMArena Bridge Error]: {error_message}"
    return format_openai_chunk(content, model, request_id)

def format_openai_non_stream_response(content: str, model: str, request_id: str, reason: str = 'stop') -> dict:
    """æ„å»ºç¬¦åˆ OpenAI è§„èŒƒçš„éæµå¼å“åº”ä½“ã€‚"""
    return {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{
            "index": 0,
            "message": {"role": "assistant", "content": content},
            "finish_reason": reason,
        }],
        "usage": {
            "prompt_tokens": 0,
            "completion_tokens": len(content) // 4,
            "total_tokens": len(content) // 4,
        },
    }

async def _process_lmarena_stream(request_id: str):
    """
    æ ¸å¿ƒå†…éƒ¨ç”Ÿæˆå™¨ï¼šå¤„ç†æ¥è‡ªæµè§ˆå™¨çš„åŸå§‹æ•°æ®æµï¼Œå¹¶äº§ç”Ÿç»“æ„åŒ–äº‹ä»¶ã€‚
    äº‹ä»¶ç±»å‹: ('content', str), ('finish', str), ('error', str)
    """
    global IS_REFRESHING_FOR_VERIFICATION
    queue = response_channels.get(request_id)
    if not queue:
        logger.error(f"PROCESSOR [ID: {request_id[:8]}]: æ— æ³•æ‰¾åˆ°å“åº”é€šé“ã€‚")
        yield 'error', 'Internal server error: response channel not found.'
        return

    buffer = ""
    timeout = CONFIG.get("stream_response_timeout_seconds",360)
    text_pattern = re.compile(r'[ab]0:"((?:\\.|[^"\\])*)"')
    # æ–°å¢ï¼šç”¨äºåŒ¹é…å’Œæå–å›¾ç‰‡URLçš„æ­£åˆ™è¡¨è¾¾å¼
    image_pattern = re.compile(r'[ab]2:(\[.*?\])')
    finish_pattern = re.compile(r'[ab]d:(\{.*?"finishReason".*?\})')
    error_pattern = re.compile(r'(\{\s*"error".*?\})', re.DOTALL)
    cloudflare_patterns = [r'<title>Just a moment...</title>', r'Enable JavaScript and cookies to continue']
    
    has_yielded_content = False # æ ‡è®°æ˜¯å¦å·²äº§å‡ºè¿‡æœ‰æ•ˆå†…å®¹

    try:
        while True:
            try:
                raw_data = await asyncio.wait_for(queue.get(), timeout=timeout)
            except asyncio.TimeoutError:
                logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: ç­‰å¾…æµè§ˆå™¨æ•°æ®è¶…æ—¶ï¼ˆ{timeout}ç§’ï¼‰ã€‚")
                yield 'error', f'Response timed out after {timeout} seconds.'
                return

            # --- Cloudflare äººæœºéªŒè¯å¤„ç† ---
            def handle_cloudflare_verification():
                global IS_REFRESHING_FOR_VERIFICATION
                if not IS_REFRESHING_FOR_VERIFICATION:
                    logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: é¦–æ¬¡æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå°†å‘é€åˆ·æ–°æŒ‡ä»¤ã€‚")
                    IS_REFRESHING_FOR_VERIFICATION = True
                    if browser_ws:
                        asyncio.create_task(browser_ws.send_text(json.dumps({"command": "refresh"}, ensure_ascii=False)))
                    return "æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œå·²å‘é€åˆ·æ–°æŒ‡ä»¤ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                else:
                    logger.info(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°äººæœºéªŒè¯ï¼Œä½†å·²åœ¨åˆ·æ–°ä¸­ï¼Œå°†ç­‰å¾…ã€‚")
                    return "æ­£åœ¨ç­‰å¾…äººæœºéªŒè¯å®Œæˆ..."

            # 1. æ£€æŸ¥æ¥è‡ª WebSocket ç«¯çš„ç›´æ¥é”™è¯¯
            if isinstance(raw_data, dict) and 'error' in raw_data:
                error_msg = raw_data.get('error', 'Unknown browser error')
                if isinstance(error_msg, str):
                    if '413' in error_msg or 'too large' in error_msg.lower():
                        friendly_error_msg = "ä¸Šä¼ å¤±è´¥ï¼šé™„ä»¶å¤§å°è¶…è¿‡äº† LMArena æœåŠ¡å™¨çš„é™åˆ¶ (é€šå¸¸æ˜¯ 5MBå·¦å³)ã€‚è¯·å°è¯•å‹ç¼©æ–‡ä»¶æˆ–ä¸Šä¼ æ›´å°çš„æ–‡ä»¶ã€‚"
                        logger.warning(f"PROCESSOR [ID: {request_id[:8]}]: æ£€æµ‹åˆ°é™„ä»¶è¿‡å¤§é”™è¯¯ (413)ã€‚")
                        yield 'error', friendly_error_msg
                        return
                    if any(re.search(p, error_msg, re.IGNORECASE) for p in cloudflare_patterns):
                        yield 'error', handle_cloudflare_verification()
                        return
                yield 'error', error_msg
                return

            # 2. æ£€æŸ¥ [DONE] ä¿¡å·
            if raw_data == "[DONE]":
                # çŠ¶æ€é‡ç½®é€»è¾‘å·²ç§»è‡³ websocket_endpointï¼Œä»¥ç¡®ä¿è¿æ¥æ¢å¤æ—¶çŠ¶æ€ä¸€å®šè¢«é‡ç½®
                if has_yielded_content and IS_REFRESHING_FOR_VERIFICATION:
                     logger.info(f"PROCESSOR [ID: {request_id[:8]}]: è¯·æ±‚æˆåŠŸï¼ŒäººæœºéªŒè¯çŠ¶æ€å°†åœ¨ä¸‹æ¬¡è¿æ¥æ—¶é‡ç½®ã€‚")
                break

            # 3. ç´¯åŠ ç¼“å†²åŒºå¹¶æ£€æŸ¥å†…å®¹
            buffer += "".join(str(item) for item in raw_data) if isinstance(raw_data, list) else raw_data

            if any(re.search(p, buffer, re.IGNORECASE) for p in cloudflare_patterns):
                yield 'error', handle_cloudflare_verification()
                return
            
            if (error_match := error_pattern.search(buffer)):
                try:
                    error_json = json.loads(error_match.group(1))
                    yield 'error', error_json.get("error", "æ¥è‡ª LMArena çš„æœªçŸ¥é”™è¯¯")
                    return
                except json.JSONDecodeError: pass

            # ä¼˜å…ˆå¤„ç†æ–‡æœ¬å†…å®¹
            while (match := text_pattern.search(buffer)):
                try:
                    text_content = json.loads(f'"{match.group(1)}"')
                    if text_content:
                        has_yielded_content = True
                        yield 'content', text_content
                except (ValueError, json.JSONDecodeError): pass
                buffer = buffer[match.end():]

            # æ–°å¢ï¼šå¤„ç†å›¾ç‰‡å†…å®¹
            while (match := image_pattern.search(buffer)):
                try:
                    image_data_list = json.loads(match.group(1))
                    if isinstance(image_data_list, list) and image_data_list:
                        image_info = image_data_list[0]
                        if image_info.get("type") == "image" and "image" in image_info:
                            # å°†URLåŒ…è£…æˆMarkdownæ ¼å¼å¹¶ä½œä¸ºå†…å®¹å—yield
                            markdown_image = f"![Image]({image_info['image']})"
                            yield 'content', markdown_image
                except (json.JSONDecodeError, IndexError) as e:
                    logger.warning(f"è§£æå›¾ç‰‡URLæ—¶å‡ºé”™: {e}, buffer: {buffer[:150]}")
                buffer = buffer[match.end():]

            if (finish_match := finish_pattern.search(buffer)):
                try:
                    finish_data = json.loads(finish_match.group(1))
                    yield 'finish', finish_data.get("finishReason", "stop")
                except (json.JSONDecodeError, IndexError): pass
                buffer = buffer[finish_match.end():]

    except asyncio.CancelledError:
        logger.info(f"PROCESSOR [ID: {request_id[:8]}]: ä»»åŠ¡è¢«å–æ¶ˆã€‚")
    finally:
        if request_id in response_channels:
            del response_channels[request_id]
            logger.info(f"PROCESSOR [ID: {request_id[:8]}]: å“åº”é€šé“å·²æ¸…ç†ã€‚")

async def stream_generator(request_id: str, model: str):
    """å°†å†…éƒ¨äº‹ä»¶æµæ ¼å¼åŒ–ä¸º OpenAI SSE å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨å¯åŠ¨ã€‚")
    
    finish_reason_to_send = 'stop'  # é»˜è®¤çš„ç»“æŸåŸå› 

    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'content':
            yield format_openai_chunk(data, model, response_id)
        elif event_type == 'finish':
            # è®°å½•ç»“æŸåŸå› ï¼Œä½†ä¸è¦ç«‹å³è¿”å›ï¼Œç­‰å¾…æµè§ˆå™¨å‘é€ [DONE]
            finish_reason_to_send = data
            if data == 'content-filter':
                warning_msg = "\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› "
                yield format_openai_chunk(warning_msg, model, response_id)
        elif event_type == 'error':
            logger.error(f"STREAMER [ID: {request_id[:8]}]: æµä¸­å‘ç”Ÿé”™è¯¯: {data}")
            yield format_openai_error_chunk(str(data), model, response_id)
            yield format_openai_finish_chunk(model, response_id, reason='stop')
            return # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œå¯ä»¥ç«‹å³ç»ˆæ­¢

    # åªæœ‰åœ¨ _process_lmarena_stream è‡ªç„¶ç»“æŸå (å³æ”¶åˆ° [DONE]) æ‰æ‰§è¡Œ
    yield format_openai_finish_chunk(model, response_id, reason=finish_reason_to_send)
    logger.info(f"STREAMER [ID: {request_id[:8]}]: æµå¼ç”Ÿæˆå™¨æ­£å¸¸ç»“æŸã€‚")

async def non_stream_response(request_id: str, model: str):
    """èšåˆå†…éƒ¨äº‹ä»¶æµå¹¶è¿”å›å•ä¸ª OpenAI JSON å“åº”ã€‚"""
    response_id = f"chatcmpl-{uuid.uuid4()}"
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å¼€å§‹å¤„ç†éæµå¼å“åº”ã€‚")
    
    full_content = []
    finish_reason = "stop"
    
    async for event_type, data in _process_lmarena_stream(request_id):
        if event_type == 'content':
            full_content.append(data)
        elif event_type == 'finish':
            finish_reason = data
            if data == 'content-filter':
                full_content.append("\n\nå“åº”è¢«ç»ˆæ­¢ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡è¶…é™æˆ–è€…æ¨¡å‹å†…éƒ¨å®¡æŸ¥ï¼ˆå¤§æ¦‚ç‡ï¼‰çš„åŸå› ")
            # ä¸è¦åœ¨è¿™é‡Œ breakï¼Œç»§ç»­ç­‰å¾…æ¥è‡ªæµè§ˆå™¨çš„ [DONE] ä¿¡å·ï¼Œä»¥é¿å…ç«æ€æ¡ä»¶
        elif event_type == 'error':
            logger.error(f"NON-STREAM [ID: {request_id[:8]}]: å¤„ç†æ—¶å‘ç”Ÿé”™è¯¯: {data}")
            
            # ç»Ÿä¸€æµå¼å’Œéæµå¼å“åº”çš„é”™è¯¯çŠ¶æ€ç 
            status_code = 413 if "é™„ä»¶å¤§å°è¶…è¿‡äº†" in str(data) else 500

            error_response = {
                "error": {
                    "message": f"[LMArena Bridge Error]: {data}",
                    "type": "bridge_error",
                    "code": "attachment_too_large" if status_code == 413 else "processing_error"
                }
            }
            return Response(content=json.dumps(error_response, ensure_ascii=False), status_code=status_code, media_type="application/json")

    final_content = "".join(full_content)
    response_data = format_openai_non_stream_response(final_content, model, response_id, reason=finish_reason)
    
    logger.info(f"NON-STREAM [ID: {request_id[:8]}]: å“åº”èšåˆå®Œæˆã€‚")
    return Response(content=json.dumps(response_data, ensure_ascii=False), media_type="application/json")

# --- WebSocket ç«¯ç‚¹ ---
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """å¤„ç†æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„ WebSocket è¿æ¥ã€‚"""
    global browser_ws, IS_REFRESHING_FOR_VERIFICATION
    await websocket.accept()
    if browser_ws is not None:
        logger.warning("æ£€æµ‹åˆ°æ–°çš„æ²¹çŒ´è„šæœ¬è¿æ¥ï¼Œæ—§çš„è¿æ¥å°†è¢«æ›¿æ¢ã€‚")
    
    # åªè¦æœ‰æ–°çš„è¿æ¥å»ºç«‹ï¼Œå°±æ„å‘³ç€äººæœºéªŒè¯æµç¨‹å·²ç»“æŸï¼ˆæˆ–ä»æœªå¼€å§‹ï¼‰
    if IS_REFRESHING_FOR_VERIFICATION:
        logger.info("âœ… æ–°çš„ WebSocket è¿æ¥å·²å»ºç«‹ï¼ŒäººæœºéªŒè¯çŠ¶æ€å·²è‡ªåŠ¨é‡ç½®ã€‚")
        IS_REFRESHING_FOR_VERIFICATION = False
        
    logger.info("âœ… æ²¹çŒ´è„šæœ¬å·²æˆåŠŸè¿æ¥ WebSocketã€‚")
    browser_ws = websocket
    try:
        while True:
            # ç­‰å¾…å¹¶æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„æ¶ˆæ¯
            message_str = await websocket.receive_text()
            message = json.loads(message_str)
            
            request_id = message.get("request_id")
            data = message.get("data")

            if not request_id or data is None:
                logger.warning(f"æ”¶åˆ°æ¥è‡ªæµè§ˆå™¨çš„æ— æ•ˆæ¶ˆæ¯: {message}")
                continue

            # å°†æ”¶åˆ°çš„æ•°æ®æ”¾å…¥å¯¹åº”çš„å“åº”é€šé“
            if request_id in response_channels:
                await response_channels[request_id].put(data)
            else:
                logger.warning(f"âš ï¸ æ”¶åˆ°æœªçŸ¥æˆ–å·²å…³é—­è¯·æ±‚çš„å“åº”: {request_id}")

    except WebSocketDisconnect:
        logger.warning("âŒ æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯å·²æ–­å¼€è¿æ¥ã€‚")
    except Exception as e:
        logger.error(f"WebSocket å¤„ç†æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
    finally:
        browser_ws = None
        # æ¸…ç†æ‰€æœ‰ç­‰å¾…çš„å“åº”é€šé“ï¼Œä»¥é˜²è¯·æ±‚è¢«æŒ‚èµ·
        for queue in response_channels.values():
            await queue.put({"error": "Browser disconnected during operation"})
        response_channels.clear()
        logger.info("WebSocket è¿æ¥å·²æ¸…ç†ã€‚")

# --- OpenAI å…¼å®¹ API ç«¯ç‚¹ ---
@app.get("/v1/models")
async def get_models():
    """æä¾›å…¼å®¹ OpenAI çš„æ¨¡å‹åˆ—è¡¨ã€‚"""
    if not MODEL_NAME_TO_ID_MAP:
        return JSONResponse(
            status_code=404,
            content={"error": "æ¨¡å‹åˆ—è¡¨ä¸ºç©ºæˆ– 'models.json' æœªæ‰¾åˆ°ã€‚"}
        )
    
    return {
        "object": "list",
        "data": [
            {
                "id": model_name, 
                "object": "model",
                "created": int(time.time()),
                "owned_by": "LMArenaBridge"
            }
            for model_name in MODEL_NAME_TO_ID_MAP.keys()
        ],
    }

@app.post("/internal/request_model_update")
async def request_model_update():
    """
    æ¥æ”¶æ¥è‡ª model_updater.py çš„è¯·æ±‚ï¼Œå¹¶é€šè¿‡ WebSocket æŒ‡ä»¤
    è®©æ²¹çŒ´è„šæœ¬å‘é€é¡µé¢æºç ã€‚
    """
    if not browser_ws:
        logger.warning("MODEL UPDATE: æ”¶åˆ°æ›´æ–°è¯·æ±‚ï¼Œä½†æ²¡æœ‰æµè§ˆå™¨è¿æ¥ã€‚")
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    
    try:
        logger.info("MODEL UPDATE: æ”¶åˆ°æ›´æ–°è¯·æ±‚ï¼Œæ­£åœ¨é€šè¿‡ WebSocket å‘é€æŒ‡ä»¤...")
        await browser_ws.send_text(json.dumps({"command": "send_page_source"}))
        logger.info("MODEL UPDATE: 'send_page_source' æŒ‡ä»¤å·²æˆåŠŸå‘é€ã€‚")
        return JSONResponse({"status": "success", "message": "Request to send page source sent."})
    except Exception as e:
        logger.error(f"MODEL UPDATE: å‘é€æŒ‡ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to send command via WebSocket.")

@app.post("/internal/update_available_models")
async def update_available_models_endpoint(request: Request):
    """
    æ¥æ”¶æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢ HTMLï¼Œæå–å¹¶æ›´æ–° available_models.jsonã€‚
    """
    html_content = await request.body()
    if not html_content:
        logger.warning("æ¨¡å‹æ›´æ–°è¯·æ±‚æœªæ”¶åˆ°ä»»ä½• HTML å†…å®¹ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "No HTML content received."}
        )
    
    logger.info("æ”¶åˆ°æ¥è‡ªæ²¹çŒ´è„šæœ¬çš„é¡µé¢å†…å®¹ï¼Œå¼€å§‹æå–å¯ç”¨æ¨¡å‹...")
    new_models_list = extract_models_from_html(html_content.decode('utf-8'))
    
    if new_models_list:
        save_available_models(new_models_list)
        return JSONResponse({"status": "success", "message": "Available models file updated."})
    else:
        logger.error("æœªèƒ½ä»æ²¹çŒ´è„šæœ¬æä¾›çš„ HTML ä¸­æå–æ¨¡å‹æ•°æ®ã€‚")
        return JSONResponse(
            status_code=400,
            content={"status": "error", "message": "Could not extract model data from HTML."}
        )


@app.post("/v1/chat/completions")
async def chat_completions(request: Request):
    """
    å¤„ç†èŠå¤©è¡¥å…¨è¯·æ±‚ã€‚
    æ¥æ”¶ OpenAI æ ¼å¼çš„è¯·æ±‚ï¼Œå°†å…¶è½¬æ¢ä¸º LMArena æ ¼å¼ï¼Œ
    é€šè¿‡ WebSocket å‘é€ç»™æ²¹çŒ´è„šæœ¬ï¼Œç„¶åæµå¼è¿”å›ç»“æœã€‚
    """
    global last_activity_time
    last_activity_time = datetime.now() # æ›´æ–°æ´»åŠ¨æ—¶é—´
    logger.info(f"APIè¯·æ±‚å·²æ”¶åˆ°ï¼Œæ´»åŠ¨æ—¶é—´å·²æ›´æ–°ä¸º: {last_activity_time.strftime('%Y-%m-%d %H:%M:%S')}")

    try:
        openai_req = await request.json()
    except json.JSONDecodeError:
        raise HTTPException(status_code=400, detail="æ— æ•ˆçš„ JSON è¯·æ±‚ä½“")

    model_name = openai_req.get("model")
    model_info = MODEL_NAME_TO_ID_MAP.get(model_name, {}) # å…³é”®ä¿®å¤ï¼šå¦‚æœæ¨¡å‹æœªæ‰¾åˆ°ï¼Œè¿”å›ä¸€ä¸ªç©ºå­—å…¸è€Œä¸æ˜¯None
    model_type = model_info.get("type", "text") # é»˜è®¤ä¸º text

    # --- æ–°å¢ï¼šåŸºäºæ¨¡å‹ç±»å‹çš„åˆ¤æ–­é€»è¾‘ ---
    if model_type == 'image':
        logger.info(f"æ£€æµ‹åˆ°æ¨¡å‹ '{model_name}' ç±»å‹ä¸º 'image'ï¼Œå°†é€šè¿‡ä¸»èŠå¤©æ¥å£å¤„ç†ã€‚")
        # å¯¹äºå›¾åƒæ¨¡å‹ï¼Œæˆ‘ä»¬ä¸å†è°ƒç”¨ç‹¬ç«‹çš„å¤„ç†å™¨ï¼Œè€Œæ˜¯å¤ç”¨ä¸»èŠå¤©é€»è¾‘ï¼Œ
        # å› ä¸º _process_lmarena_stream ç°åœ¨å·²ç»èƒ½å¤„ç†å›¾ç‰‡æ•°æ®ã€‚
        # è¿™æ„å‘³ç€å›¾åƒç”Ÿæˆç°åœ¨åŸç”Ÿæ”¯æŒæµå¼å’Œéæµå¼å“åº”ã€‚
        pass # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„é€šç”¨èŠå¤©é€»è¾‘
    # --- æ–‡ç”Ÿå›¾é€»è¾‘ç»“æŸ ---

    # å¦‚æœä¸æ˜¯å›¾åƒæ¨¡å‹ï¼Œåˆ™æ‰§è¡Œæ­£å¸¸çš„æ–‡æœ¬ç”Ÿæˆé€»è¾‘
    load_config()  # å®æ—¶åŠ è½½æœ€æ–°é…ç½®ï¼Œç¡®ä¿ä¼šè¯IDç­‰ä¿¡æ¯æ˜¯æœ€æ–°çš„
    # --- API Key éªŒè¯ ---
    api_key = CONFIG.get("api_key")
    if api_key:
        auth_header = request.headers.get('Authorization')
        if not auth_header or not auth_header.startswith('Bearer '):
            raise HTTPException(
                status_code=401,
                detail="æœªæä¾› API Keyã€‚è¯·åœ¨ Authorization å¤´éƒ¨ä¸­ä»¥ 'Bearer YOUR_KEY' æ ¼å¼æä¾›ã€‚"
            )
        
        provided_key = auth_header.split(' ')[1]
        if provided_key != api_key:
            raise HTTPException(
                status_code=401,
                detail="æä¾›çš„ API Key ä¸æ­£ç¡®ã€‚"
            )

    # --- å¢å¼ºçš„è¿æ¥æ£€æŸ¥ï¼Œè§£å†³äººæœºéªŒè¯åçš„ç«æ€æ¡ä»¶ ---
    if IS_REFRESHING_FOR_VERIFICATION and not browser_ws:
        raise HTTPException(
            status_code=503,
            detail="æ­£åœ¨ç­‰å¾…æµè§ˆå™¨åˆ·æ–°ä»¥å®ŒæˆäººæœºéªŒè¯ï¼Œè¯·åœ¨å‡ ç§’é’Ÿåé‡è¯•ã€‚"
        )

    if not browser_ws:
        raise HTTPException(
            status_code=503,
            detail="æ²¹çŒ´è„šæœ¬å®¢æˆ·ç«¯æœªè¿æ¥ã€‚è¯·ç¡®ä¿ LMArena é¡µé¢å·²æ‰“å¼€å¹¶æ¿€æ´»è„šæœ¬ã€‚"
        )

    # --- æ¨¡å‹ä¸ä¼šè¯IDæ˜ å°„é€»è¾‘ ---
    session_id, message_id = None, None
    mode_override, battle_target_override = None, None

    if model_name and model_name in MODEL_ENDPOINT_MAP:
        mapping_entry = MODEL_ENDPOINT_MAP[model_name]
        selected_mapping = None

        if isinstance(mapping_entry, list) and mapping_entry:
            selected_mapping = random.choice(mapping_entry)
            logger.info(f"ä¸ºæ¨¡å‹ '{model_name}' ä»IDåˆ—è¡¨ä¸­éšæœºé€‰æ‹©äº†ä¸€ä¸ªæ˜ å°„ã€‚")
        elif isinstance(mapping_entry, dict):
            selected_mapping = mapping_entry
            logger.info(f"ä¸ºæ¨¡å‹ '{model_name}' æ‰¾åˆ°äº†å•ä¸ªç«¯ç‚¹æ˜ å°„ï¼ˆæ—§æ ¼å¼ï¼‰ã€‚")
        
        if selected_mapping:
            session_id = selected_mapping.get("session_id")
            message_id = selected_mapping.get("message_id")
            # å…³é”®ï¼šåŒæ—¶è·å–æ¨¡å¼ä¿¡æ¯
            mode_override = selected_mapping.get("mode") # å¯èƒ½ä¸º None
            battle_target_override = selected_mapping.get("battle_target") # å¯èƒ½ä¸º None
            log_msg = f"å°†ä½¿ç”¨ Session ID: ...{session_id[-6:] if session_id else 'N/A'}"
            if mode_override:
                log_msg += f" (æ¨¡å¼: {mode_override}"
                if mode_override == 'battle':
                    log_msg += f", ç›®æ ‡: {battle_target_override or 'A'}"
                log_msg += ")"
            logger.info(log_msg)

    # å¦‚æœç»è¿‡ä»¥ä¸Šå¤„ç†ï¼Œsession_id ä»ç„¶æ˜¯ Noneï¼Œåˆ™è¿›å…¥å…¨å±€å›é€€é€»è¾‘
    if not session_id:
        if CONFIG.get("use_default_ids_if_mapping_not_found", True):
            session_id = CONFIG.get("session_id")
            message_id = CONFIG.get("message_id")
            # å½“ä½¿ç”¨å…¨å±€IDæ—¶ï¼Œä¸è®¾ç½®æ¨¡å¼è¦†ç›–ï¼Œè®©å…¶ä½¿ç”¨å…¨å±€é…ç½®
            mode_override, battle_target_override = None, None
            logger.info(f"æ¨¡å‹ '{model_name}' æœªæ‰¾åˆ°æœ‰æ•ˆæ˜ å°„ï¼Œæ ¹æ®é…ç½®ä½¿ç”¨å…¨å±€é»˜è®¤ Session ID: ...{session_id[-6:] if session_id else 'N/A'}")
        else:
            logger.error(f"æ¨¡å‹ '{model_name}' æœªåœ¨ 'model_endpoint_map.json' ä¸­æ‰¾åˆ°æœ‰æ•ˆæ˜ å°„ï¼Œä¸”å·²ç¦ç”¨å›é€€åˆ°é»˜è®¤IDã€‚")
            raise HTTPException(
                status_code=400,
                detail=f"æ¨¡å‹ '{model_name}' æ²¡æœ‰é…ç½®ç‹¬ç«‹çš„ä¼šè¯IDã€‚è¯·åœ¨ 'model_endpoint_map.json' ä¸­æ·»åŠ æœ‰æ•ˆæ˜ å°„æˆ–åœ¨ 'config.jsonc' ä¸­å¯ç”¨ 'use_default_ids_if_mapping_not_found'ã€‚"
            )

    # --- éªŒè¯æœ€ç»ˆç¡®å®šçš„ä¼šè¯ä¿¡æ¯ ---
    if not session_id or not message_id or "YOUR_" in session_id or "YOUR_" in message_id:
        raise HTTPException(
            status_code=400,
            detail="æœ€ç»ˆç¡®å®šçš„ä¼šè¯IDæˆ–æ¶ˆæ¯IDæ— æ•ˆã€‚è¯·æ£€æŸ¥ 'model_endpoint_map.json' å’Œ 'config.jsonc' ä¸­çš„é…ç½®ï¼Œæˆ–è¿è¡Œ `id_updater.py` æ¥æ›´æ–°é»˜è®¤å€¼ã€‚"
        )

    if not model_name or model_name not in MODEL_NAME_TO_ID_MAP:
        logger.warning(f"è¯·æ±‚çš„æ¨¡å‹ '{model_name}' ä¸åœ¨ models.json ä¸­ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¨¡å‹IDã€‚")

    request_id = str(uuid.uuid4())
    response_channels[request_id] = asyncio.Queue()
    logger.info(f"API CALL [ID: {request_id[:8]}]: å·²åˆ›å»ºå“åº”é€šé“ã€‚")

    try:
        # --- é™„ä»¶é¢„å¤„ç†ï¼ˆåŒ…æ‹¬æ–‡ä»¶åºŠä¸Šä¼ ï¼‰ ---
        # åœ¨ä¸æµè§ˆå™¨é€šä¿¡å‰ï¼Œå…ˆå¤„ç†å¥½æ‰€æœ‰é™„ä»¶ã€‚å¦‚æœå¤±è´¥ï¼Œåˆ™ç«‹å³è¿”å›é”™è¯¯ã€‚
        messages_to_process = openai_req.get("messages", [])
        for message in messages_to_process:
            content = message.get("content")
            if isinstance(content, list):
                for i, part in enumerate(content):
                    if part.get("type") == "image_url" and CONFIG.get("file_bed_enabled"):
                        image_url_data = part.get("image_url", {})
                        base64_url = image_url_data.get("url")
                        original_filename = image_url_data.get("detail")
                        
                        if not (base64_url and base64_url.startswith("data:")):
                            raise ValueError(f"æ— æ•ˆçš„å›¾ç‰‡æ•°æ®æ ¼å¼: {base64_url[:100] if base64_url else 'None'}")

                        upload_url = CONFIG.get("file_bed_upload_url")
                        if not upload_url:
                            raise ValueError("æ–‡ä»¶åºŠå·²å¯ç”¨ï¼Œä½† 'file_bed_upload_url' æœªé…ç½®ã€‚")
                        
                        # ç¡®ä¿å¤„ç†è½¬ä¹‰çš„æ–œæ 
                        upload_url = upload_url.replace('\\/', '/')

                        api_key = CONFIG.get("file_bed_api_key")
                        file_name = original_filename or f"image_{uuid.uuid4()}.png"
                        
                        logger.info(f"æ–‡ä»¶åºŠé¢„å¤„ç†ï¼šæ­£åœ¨ä¸Šä¼  '{file_name}'...")
                        uploaded_filename, error_message = await upload_to_file_bed(file_name, base64_url, upload_url, api_key)

                        if error_message:
                            raise IOError(f"æ–‡ä»¶åºŠä¸Šä¼ å¤±è´¥: {error_message}")
                        
                        # æ ¹æ®æ‚¨çš„å»ºè®®ï¼Œä½¿ç”¨ config ä¸­çš„ URL å‰ç¼€æ„å»ºæœ€ç»ˆ URL
                        url_prefix = upload_url.rsplit('/', 1)[0]
                        final_url = f"{url_prefix}/uploads/{uploaded_filename}"
                        
                        part["image_url"]["url"] = final_url
                        logger.info(f"é™„ä»¶URLå·²æˆåŠŸæ›¿æ¢ä¸º: {final_url}")

        # 1. è½¬æ¢è¯·æ±‚ (æ­¤æ—¶å·²ä¸åŒ…å«éœ€è¦ä¸Šä¼ çš„é™„ä»¶)
        lmarena_payload = await convert_openai_to_lmarena_payload(
            openai_req,
            session_id,
            message_id,
            mode_override=mode_override,
            battle_target_override=battle_target_override
        )
        
        # å…³é”®è¡¥å……ï¼šå¦‚æœæ¨¡å‹æ˜¯å›¾ç‰‡ç±»å‹ï¼Œåˆ™å‘æ²¹çŒ´è„šæœ¬æ˜ç¡®æŒ‡å‡º
        if model_type == 'image':
            lmarena_payload['is_image_request'] = True
        
        # 2. åŒ…è£…æˆå‘é€ç»™æµè§ˆå™¨çš„æ¶ˆæ¯
        message_to_browser = {
            "request_id": request_id,
            "payload": lmarena_payload
        }
        
        # 3. é€šè¿‡ WebSocket å‘é€
        logger.info(f"API CALL [ID: {request_id[:8]}]: æ­£åœ¨é€šè¿‡ WebSocket å‘é€è½½è·åˆ°æ²¹çŒ´è„šæœ¬ã€‚")
        await browser_ws.send_text(json.dumps(message_to_browser))

        # 4. æ ¹æ® stream å‚æ•°å†³å®šè¿”å›ç±»å‹
        is_stream = openai_req.get("stream", False)

        if is_stream:
            # è¿”å›æµå¼å“åº”
            return StreamingResponse(
                stream_generator(request_id, model_name or "default_model"),
                media_type="text/event-stream"
            )
        else:
            # è¿”å›éæµå¼å“åº”
            return await non_stream_response(request_id, model_name or "default_model")
    except (ValueError, IOError) as e:
        # æ•è·é™„ä»¶å¤„ç†é”™è¯¯
        logger.error(f"API CALL [ID: {request_id[:8]}]: é™„ä»¶é¢„å¤„ç†å¤±è´¥: {e}")
        if request_id in response_channels:
            del response_channels[request_id]
        # è¿”å›ä¸€ä¸ªæ ¼å¼æ­£ç¡®çš„JSONé”™è¯¯å“åº”
        return JSONResponse(
            status_code=500,
            content={"error": {"message": f"[LMArena Bridge Error] é™„ä»¶å¤„ç†å¤±è´¥: {e}", "type": "attachment_error"}}
        )
    except Exception as e:
        # æ•è·æ‰€æœ‰å…¶ä»–é”™è¯¯
        if request_id in response_channels:
            del response_channels[request_id]
        logger.error(f"API CALL [ID: {request_id[:8]}]: å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿè‡´å‘½é”™è¯¯: {e}", exc_info=True)
        # ç¡®ä¿ä¹Ÿè¿”å›æ ¼å¼æ­£ç¡®çš„JSON
        return JSONResponse(
            status_code=500,
            content={"error": {"message": str(e), "type": "internal_server_error"}}
        )

# --- å†…éƒ¨é€šä¿¡ç«¯ç‚¹ ---
@app.post("/internal/start_id_capture")
async def start_id_capture():
    """
    æ¥æ”¶æ¥è‡ª id_updater.py çš„é€šçŸ¥ï¼Œå¹¶é€šè¿‡ WebSocket æŒ‡ä»¤
    æ¿€æ´»æ²¹çŒ´è„šæœ¬çš„ ID æ•è·æ¨¡å¼ã€‚
    """
    if not browser_ws:
        logger.warning("ID CAPTURE: æ”¶åˆ°æ¿€æ´»è¯·æ±‚ï¼Œä½†æ²¡æœ‰æµè§ˆå™¨è¿æ¥ã€‚")
        raise HTTPException(status_code=503, detail="Browser client not connected.")
    
    try:
        logger.info("ID CAPTURE: æ”¶åˆ°æ¿€æ´»è¯·æ±‚ï¼Œæ­£åœ¨é€šè¿‡ WebSocket å‘é€æŒ‡ä»¤...")
        await browser_ws.send_text(json.dumps({"command": "activate_id_capture"}))
        logger.info("ID CAPTURE: æ¿€æ´»æŒ‡ä»¤å·²æˆåŠŸå‘é€ã€‚")
        return JSONResponse({"status": "success", "message": "Activation command sent."})
    except Exception as e:
        logger.error(f"ID CAPTURE: å‘é€æ¿€æ´»æŒ‡ä»¤æ—¶å‡ºé”™: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Failed to send command via WebSocket.")


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    # å»ºè®®ä» config.jsonc ä¸­è¯»å–ç«¯å£ï¼Œæ­¤å¤„ä¸ºä¸´æ—¶ç¡¬ç¼–ç 
    api_port = 5102
    logger.info(f"ğŸš€ LMArena Bridge v2.0 API æœåŠ¡å™¨æ­£åœ¨å¯åŠ¨...")
    logger.info(f"   - ç›‘å¬åœ°å€: http://127.0.0.1:{api_port}")
    logger.info(f"   - WebSocket ç«¯ç‚¹: ws://127.0.0.1:{api_port}/ws")
    
    uvicorn.run(app, host="0.0.0.0", port=api_port)
